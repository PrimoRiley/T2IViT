{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb#scrollTo=7tjOWPQYLq4u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gotta get those dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ImageGPTFeatureExtractor, ImageGPTModel\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alright, so I found a model that can give us the pretrained benefits while not loosing information from scaling up the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\models\\imagegpt\\feature_extraction_imagegpt.py:28: FutureWarning: The class ImageGPTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ImageGPTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at openai/imagegpt-small were not used when initializing ImageGPTModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing ImageGPTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ImageGPTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageGPTModel(\n",
       "  (wte): Embedding(513, 512)\n",
       "  (wpe): Embedding(1024, 512)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-23): 24 x ImageGPTBlock(\n",
       "      (ln_1): ImageGPTLayerNorm()\n",
       "      (attn): ImageGPTAttention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): ImageGPTLayerNorm()\n",
       "      (mlp): ImageGPTMLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): QuickGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): ImageGPTLayerNorm()\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_checkpoint = 'openai/imagegpt-small'\n",
    "feature_extractor = ImageGPTFeatureExtractor.from_pretrained(\"openai/imagegpt-small\")\n",
    "ImageGPT = ImageGPTModel.from_pretrained(\"openai/imagegpt-small\")\n",
    "ImageGPT.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this version of the model just outputs hidden states so we need to create a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3  \n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)   # Fully connected layer 1\n",
    "        self.fc2 = nn.Linear(128, num_classes)  # Fully connected layer 2 (output layer)\n",
    "        self.softmax = nn.Softmax(dim=1)        # Softmax activation for multi-class classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))    # Apply ReLU activation to the first fully connected layer\n",
    "        x = self.fc2(x)                # Get the logits (scores) for each class\n",
    "        output = self.softmax(x)       # Apply softmax to get class probabilities\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this combines ImageGPT with new classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VIT, self).__init__()\n",
    "        # Load or initialize the ImageGPT model\n",
    "        self.image_gpt = ImageGPTModel.from_pretrained(\"openai/imagegpt-small\")\n",
    "        \n",
    "        # Define the classification head\n",
    "        self.classification_head = ClassificationHead(self.image_gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # images = torch.tensor(images).to(torch.int64)\n",
    "        print(f'imgae type: {type(images)}')\n",
    "        # images = torch.tensor(images).to(device).long()\n",
    "        # Apparently this adresses the bug \n",
    "\n",
    "        images = images.type(torch.LongTensor)\n",
    "        b_input_mask = b_input_mask.type(torch.LongTensor)\n",
    "        b_labels = b_labels.type(torch.LongTensor)\n",
    "\n",
    "        b_input_ids = b_input_ids.to(device)\n",
    "        b_input_mask = b_input_mask.to(device)\n",
    "        b_labels = b_labels.to(device)\n",
    "\n",
    "\n",
    "        # Pass the images through the ImageGPT model\n",
    "        features = self.image_gpt(images)\n",
    "\n",
    "        # Pass the pooled output through the classification head\n",
    "        output = self.classification_head(features)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here I need to load the data in a compatible way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/wine_data/images-standard/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Resolving data files: 100%|██████████| 170/170 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 170\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "# load a custom dataset from local/remote files or folders using the ImageFolder feature\n",
    "\n",
    "# option 1: local/remote files (supporting the following formats: tar, gzip, zip, xz, rar, zstd)\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=data_dir)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id2string dictionary to decode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "id2label[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageGPTImageProcessor {\n",
       "  \"clusters\": [\n",
       "    [\n",
       "      0.8866443634033203,\n",
       "      0.6618829369544983,\n",
       "      0.3891746401786804\n",
       "    ],\n",
       "    [\n",
       "      -0.6042559146881104,\n",
       "      -0.02295008860528469,\n",
       "      0.5423797369003296\n",
       "    ],\n",
       "    [\n",
       "      0.12942790985107422,\n",
       "      0.03570118546485901,\n",
       "      -0.3643915355205536\n",
       "    ],\n",
       "    [\n",
       "      -0.3553103804588318,\n",
       "      -0.15857496857643127,\n",
       "      -0.664303183555603\n",
       "    ],\n",
       "    [\n",
       "      0.7844981551170349,\n",
       "      -0.2559399902820587,\n",
       "      -0.7189618945121765\n",
       "    ],\n",
       "    [\n",
       "      0.1935412883758545,\n",
       "      0.23239648342132568,\n",
       "      0.08234082162380219\n",
       "    ],\n",
       "    [\n",
       "      -0.9012048840522766,\n",
       "      -0.7926875352859497,\n",
       "      -0.9216221570968628\n",
       "    ],\n",
       "    [\n",
       "      -0.013770446181297302,\n",
       "      -0.8677454590797424,\n",
       "      -0.8910260796546936\n",
       "    ],\n",
       "    [\n",
       "      -0.8732763528823853,\n",
       "      -0.5560516715049744,\n",
       "      -0.686274528503418\n",
       "    ],\n",
       "    [\n",
       "      -0.3679597079753876,\n",
       "      -0.4616401493549347,\n",
       "      -0.8443512916564941\n",
       "    ],\n",
       "    [\n",
       "      -0.17434515058994293,\n",
       "      -0.35372495651245117,\n",
       "      -0.5616933107376099\n",
       "    ],\n",
       "    [\n",
       "      0.7205567955970764,\n",
       "      0.6045222282409668,\n",
       "      0.527334988117218\n",
       "    ],\n",
       "    [\n",
       "      -0.05053819343447685,\n",
       "      0.1381247639656067,\n",
       "      -0.1910051703453064\n",
       "    ],\n",
       "    [\n",
       "      0.53249192237854,\n",
       "      0.21986378729343414,\n",
       "      -0.1370566189289093\n",
       "    ],\n",
       "    [\n",
       "      -0.5499509572982788,\n",
       "      0.6311273574829102,\n",
       "      0.8301469683647156\n",
       "    ],\n",
       "    [\n",
       "      -0.0623948909342289,\n",
       "      -0.10504596680402756,\n",
       "      -0.2894057631492615\n",
       "    ],\n",
       "    [\n",
       "      0.8541644215583801,\n",
       "      0.4792228937149048,\n",
       "      0.07953768968582153\n",
       "    ],\n",
       "    [\n",
       "      -0.4161697328090668,\n",
       "      -0.5115739703178406,\n",
       "      -0.43697986006736755\n",
       "    ],\n",
       "    [\n",
       "      0.9533290266990662,\n",
       "      0.9274855852127075,\n",
       "      0.5679895877838135\n",
       "    ],\n",
       "    [\n",
       "      -0.7291542291641235,\n",
       "      -0.628097653388977,\n",
       "      -0.7699974775314331\n",
       "    ],\n",
       "    [\n",
       "      0.5313871502876282,\n",
       "      0.26622718572616577,\n",
       "      0.2107263058423996\n",
       "    ],\n",
       "    [\n",
       "      0.02250233106315136,\n",
       "      -0.1575583517551422,\n",
       "      -0.09264706820249557\n",
       "    ],\n",
       "    [\n",
       "      -0.16142235696315765,\n",
       "      0.0030967944767326117,\n",
       "      0.680765688419342\n",
       "    ],\n",
       "    [\n",
       "      -0.0008921666303649545,\n",
       "      -0.047903429716825485,\n",
       "      -0.11226630955934525\n",
       "    ],\n",
       "    [\n",
       "      -0.10056599974632263,\n",
       "      -0.6199851632118225,\n",
       "      -0.5905289649963379\n",
       "    ],\n",
       "    [\n",
       "      0.36189737915992737,\n",
       "      -0.5128692984580994,\n",
       "      -0.5751007199287415\n",
       "    ],\n",
       "    [\n",
       "      -0.5218700766563416,\n",
       "      -0.35565218329429626,\n",
       "      -0.6801741719245911\n",
       "    ],\n",
       "    [\n",
       "      -0.004487038590013981,\n",
       "      0.0499449297785759,\n",
       "      -0.34433433413505554\n",
       "    ],\n",
       "    [\n",
       "      0.3216712176799774,\n",
       "      0.2629038691520691,\n",
       "      0.09036855399608612\n",
       "    ],\n",
       "    [\n",
       "      0.27530282735824585,\n",
       "      0.40375596284866333,\n",
       "      -0.4079127311706543\n",
       "    ],\n",
       "    [\n",
       "      -0.4074484705924988,\n",
       "      -0.26290711760520935,\n",
       "      -0.33841392397880554\n",
       "    ],\n",
       "    [\n",
       "      0.7986500859260559,\n",
       "      0.7467415928840637,\n",
       "      0.647467315196991\n",
       "    ],\n",
       "    [\n",
       "      -0.16817767918109894,\n",
       "      0.4910394251346588,\n",
       "      0.15420623123645782\n",
       "    ],\n",
       "    [\n",
       "      -0.21436354517936707,\n",
       "      0.023510200902819633,\n",
       "      0.43241268396377563\n",
       "    ],\n",
       "    [\n",
       "      -0.40356260538101196,\n",
       "      -0.5356048941612244,\n",
       "      -0.5896926522254944\n",
       "    ],\n",
       "    [\n",
       "      0.3242187798023224,\n",
       "      0.08525704592466354,\n",
       "      -0.19164560735225677\n",
       "    ],\n",
       "    [\n",
       "      -0.32216736674308777,\n",
       "      -0.4917454123497009,\n",
       "      -0.6309071779251099\n",
       "    ],\n",
       "    [\n",
       "      -0.38158607482910156,\n",
       "      -0.5385871529579163,\n",
       "      -0.7054222822189331\n",
       "    ],\n",
       "    [\n",
       "      -0.24477025866508484,\n",
       "      0.07710830122232437,\n",
       "      -0.21819734573364258\n",
       "    ],\n",
       "    [\n",
       "      -0.4544757604598999,\n",
       "      -0.28715264797210693,\n",
       "      0.19457608461380005\n",
       "    ],\n",
       "    [\n",
       "      0.3922958970069885,\n",
       "      -0.04505942016839981,\n",
       "      -0.46043047308921814\n",
       "    ],\n",
       "    [\n",
       "      -0.6633502840995789,\n",
       "      -0.4843274652957916,\n",
       "      -0.24858099222183228\n",
       "    ],\n",
       "    [\n",
       "      0.3302103877067566,\n",
       "      0.5696513056755066,\n",
       "      0.10619977116584778\n",
       "    ],\n",
       "    [\n",
       "      -0.1213420107960701,\n",
       "      -0.1513548493385315,\n",
       "      -0.7746183276176453\n",
       "    ],\n",
       "    [\n",
       "      0.02183908224105835,\n",
       "      -0.29862523078918457,\n",
       "      -0.5074599981307983\n",
       "    ],\n",
       "    [\n",
       "      -0.8835203051567078,\n",
       "      -0.7302054762840271,\n",
       "      0.30284789204597473\n",
       "    ],\n",
       "    [\n",
       "      -0.410575807094574,\n",
       "      -0.10831455886363983,\n",
       "      0.28231197595596313\n",
       "    ],\n",
       "    [\n",
       "      -0.0038531296886503696,\n",
       "      -0.3962152898311615,\n",
       "      -0.7741573452949524\n",
       "    ],\n",
       "    [\n",
       "      0.34942829608917236,\n",
       "      0.350960910320282,\n",
       "      0.3488500118255615\n",
       "    ],\n",
       "    [\n",
       "      -0.39364781975746155,\n",
       "      0.03495653346180916,\n",
       "      -0.3540157973766327\n",
       "    ],\n",
       "    [\n",
       "      -0.07268346846103668,\n",
       "      0.029805386438965797,\n",
       "      0.30300232768058777\n",
       "    ],\n",
       "    [\n",
       "      -0.855398416519165,\n",
       "      -0.5722979307174683,\n",
       "      -0.19805879890918732\n",
       "    ],\n",
       "    [\n",
       "      -0.26683011651039124,\n",
       "      -0.27701082825660706,\n",
       "      -0.2609962224960327\n",
       "    ],\n",
       "    [\n",
       "      -0.3457053005695343,\n",
       "      0.2817457616329193,\n",
       "      -0.5590637922286987\n",
       "    ],\n",
       "    [\n",
       "      -0.6952950954437256,\n",
       "      -0.5743372440338135,\n",
       "      -0.6705573201179504\n",
       "    ],\n",
       "    [\n",
       "      0.523847758769989,\n",
       "      0.32931753993034363,\n",
       "      -0.029757514595985413\n",
       "    ],\n",
       "    [\n",
       "      0.4000224769115448,\n",
       "      0.252950519323349,\n",
       "      -0.07705940306186676\n",
       "    ],\n",
       "    [\n",
       "      -0.21261973679065704,\n",
       "      -0.1472283899784088,\n",
       "      0.2760908603668213\n",
       "    ],\n",
       "    [\n",
       "      -0.5430170893669128,\n",
       "      -0.3718504011631012,\n",
       "      0.012515963986515999\n",
       "    ],\n",
       "    [\n",
       "      -0.92361980676651,\n",
       "      -0.9171434640884399,\n",
       "      -0.9339240193367004\n",
       "    ],\n",
       "    [\n",
       "      -0.17573337256908417,\n",
       "      -0.1685536950826645,\n",
       "      -0.33348602056503296\n",
       "    ],\n",
       "    [\n",
       "      -0.30869874358177185,\n",
       "      -0.3544236123561859,\n",
       "      -0.1135273203253746\n",
       "    ],\n",
       "    [\n",
       "      -0.5163084268569946,\n",
       "      0.05249231308698654,\n",
       "      -0.7978107333183289\n",
       "    ],\n",
       "    [\n",
       "      -0.1575734168291092,\n",
       "      -0.26073628664016724,\n",
       "      -0.29685407876968384\n",
       "    ],\n",
       "    [\n",
       "      -0.07752817869186401,\n",
       "      -0.10982697457075119,\n",
       "      0.06651923060417175\n",
       "    ],\n",
       "    [\n",
       "      -0.5863983631134033,\n",
       "      -0.16673429310321808,\n",
       "      -0.3866238296031952\n",
       "    ],\n",
       "    [\n",
       "      -0.28994566202163696,\n",
       "      -0.20120011270046234,\n",
       "      0.05026695132255554\n",
       "    ],\n",
       "    [\n",
       "      -0.793811023235321,\n",
       "      -0.2712944746017456,\n",
       "      0.7479417324066162\n",
       "    ],\n",
       "    [\n",
       "      -0.4872121810913086,\n",
       "      0.26298385858535767,\n",
       "      0.6379027366638184\n",
       "    ],\n",
       "    [\n",
       "      0.10957116633653641,\n",
       "      -0.028541117906570435,\n",
       "      0.10372563451528549\n",
       "    ],\n",
       "    [\n",
       "      -0.4547993242740631,\n",
       "      -0.4405229091644287,\n",
       "      -0.47519412636756897\n",
       "    ],\n",
       "    [\n",
       "      -0.9882778525352478,\n",
       "      -0.9892479777336121,\n",
       "      -0.9887582063674927\n",
       "    ],\n",
       "    [\n",
       "      0.8967511057853699,\n",
       "      0.8172797560691833,\n",
       "      -0.00848731491714716\n",
       "    ],\n",
       "    [\n",
       "      0.8169391751289368,\n",
       "      0.20236308872699738,\n",
       "      0.22856123745441437\n",
       "    ],\n",
       "    [\n",
       "      0.9754669666290283,\n",
       "      0.9802407026290894,\n",
       "      0.9766666293144226\n",
       "    ],\n",
       "    [\n",
       "      -0.9453840255737305,\n",
       "      -0.9282575845718384,\n",
       "      -0.8594306707382202\n",
       "    ],\n",
       "    [\n",
       "      -0.9195259809494019,\n",
       "      -0.8593184947967529,\n",
       "      -0.7556572556495667\n",
       "    ],\n",
       "    [\n",
       "      -0.8385721445083618,\n",
       "      -0.190311461687088,\n",
       "      -0.11632368713617325\n",
       "    ],\n",
       "    [\n",
       "      -0.43058156967163086,\n",
       "      -0.22221753001213074,\n",
       "      -0.5097092986106873\n",
       "    ],\n",
       "    [\n",
       "      -0.22450797259807587,\n",
       "      -0.06987760215997696,\n",
       "      0.10926814377307892\n",
       "    ],\n",
       "    [\n",
       "      -0.9219630360603333,\n",
       "      -0.8413735628128052,\n",
       "      -0.6118304133415222\n",
       "    ],\n",
       "    [\n",
       "      0.9968767166137695,\n",
       "      0.9970247745513916,\n",
       "      0.9967049956321716\n",
       "    ],\n",
       "    [\n",
       "      0.205012708902359,\n",
       "      -0.4318155348300934,\n",
       "      -0.8442745208740234\n",
       "    ],\n",
       "    [\n",
       "      -0.09030798822641373,\n",
       "      -0.34330934286117554,\n",
       "      -0.47994205355644226\n",
       "    ],\n",
       "    [\n",
       "      0.9550600647926331,\n",
       "      0.9519978761672974,\n",
       "      0.9411756992340088\n",
       "    ],\n",
       "    [\n",
       "      0.13809911906719208,\n",
       "      -0.21429237723350525,\n",
       "      -0.5639499425888062\n",
       "    ],\n",
       "    [\n",
       "      -0.2655453383922577,\n",
       "      -0.36435338854789734,\n",
       "      -0.5013294816017151\n",
       "    ],\n",
       "    [\n",
       "      0.16679424047470093,\n",
       "      0.167513906955719,\n",
       "      0.15711624920368195\n",
       "    ],\n",
       "    [\n",
       "      -0.36666491627693176,\n",
       "      -0.685397744178772,\n",
       "      -0.8884972333908081\n",
       "    ],\n",
       "    [\n",
       "      -0.5366653800010681,\n",
       "      -0.5366100668907166,\n",
       "      -0.5332698225975037\n",
       "    ],\n",
       "    [\n",
       "      -0.00972462072968483,\n",
       "      0.12029983103275299,\n",
       "      0.48295629024505615\n",
       "    ],\n",
       "    [\n",
       "      0.2542765140533447,\n",
       "      0.2748924493789673,\n",
       "      -0.032631732523441315\n",
       "    ],\n",
       "    [\n",
       "      -0.5462303757667542,\n",
       "      -0.525596559047699,\n",
       "      -0.8729211688041687\n",
       "    ],\n",
       "    [\n",
       "      0.5557466149330139,\n",
       "      0.058265406638383865,\n",
       "      0.15023870766162872\n",
       "    ],\n",
       "    [\n",
       "      0.0680597722530365,\n",
       "      0.026245953515172005,\n",
       "      -0.5551841259002686\n",
       "    ],\n",
       "    [\n",
       "      0.6587851643562317,\n",
       "      0.5081396698951721,\n",
       "      -0.4627259373664856\n",
       "    ],\n",
       "    [\n",
       "      -0.9546892046928406,\n",
       "      -0.9589608311653137,\n",
       "      -0.9571543335914612\n",
       "    ],\n",
       "    [\n",
       "      -0.4646446108818054,\n",
       "      -0.4912768006324768,\n",
       "      -0.5407620668411255\n",
       "    ],\n",
       "    [\n",
       "      -0.4812314510345459,\n",
       "      -0.6482282280921936,\n",
       "      -0.5272032022476196\n",
       "    ],\n",
       "    [\n",
       "      0.12023928016424179,\n",
       "      -0.5664827227592468,\n",
       "      -0.6074532270431519\n",
       "    ],\n",
       "    [\n",
       "      0.6493494510650635,\n",
       "      0.9341198205947876,\n",
       "      0.9682050943374634\n",
       "    ],\n",
       "    [\n",
       "      -0.49569153785705566,\n",
       "      -0.33036357164382935,\n",
       "      -0.42618829011917114\n",
       "    ],\n",
       "    [\n",
       "      -0.73708575963974,\n",
       "      -0.6543773412704468,\n",
       "      -0.5806164741516113\n",
       "    ],\n",
       "    [\n",
       "      0.009461924433708191,\n",
       "      0.2977884113788605,\n",
       "      -0.76119464635849\n",
       "    ],\n",
       "    [\n",
       "      0.6656442284584045,\n",
       "      0.6677116751670837,\n",
       "      0.66777503490448\n",
       "    ],\n",
       "    [\n",
       "      0.15271037817001343,\n",
       "      0.1402694284915924,\n",
       "      -0.045931797474622726\n",
       "    ],\n",
       "    [\n",
       "      -0.21469759941101074,\n",
       "      -0.21596451103687286,\n",
       "      -0.24124926328659058\n",
       "    ],\n",
       "    [\n",
       "      0.033514540642499924,\n",
       "      0.1876029521226883,\n",
       "      -0.2994413673877716\n",
       "    ],\n",
       "    [\n",
       "      0.6557291746139526,\n",
       "      0.2525959312915802,\n",
       "      -0.03010040521621704\n",
       "    ],\n",
       "    [\n",
       "      0.3780847489833832,\n",
       "      0.22955067455768585,\n",
       "      0.29666024446487427\n",
       "    ],\n",
       "    [\n",
       "      0.09473847597837448,\n",
       "      -0.2077258676290512,\n",
       "      -0.4256230294704437\n",
       "    ],\n",
       "    [\n",
       "      -0.1965586394071579,\n",
       "      -0.6026338934898376,\n",
       "      -0.877092719078064\n",
       "    ],\n",
       "    [\n",
       "      -0.22824859619140625,\n",
       "      -0.060569919645786285,\n",
       "      -0.3626217246055603\n",
       "    ],\n",
       "    [\n",
       "      0.4986802935600281,\n",
       "      0.5413235425949097,\n",
       "      0.6382442116737366\n",
       "    ],\n",
       "    [\n",
       "      -0.2982630133628845,\n",
       "      -0.44031649827957153,\n",
       "      -0.3191494941711426\n",
       "    ],\n",
       "    [\n",
       "      0.39659959077835083,\n",
       "      0.40043067932128906,\n",
       "      0.3959878385066986\n",
       "    ],\n",
       "    [\n",
       "      -0.08158642798662186,\n",
       "      -0.019053487107157707,\n",
       "      -0.6247203350067139\n",
       "    ],\n",
       "    [\n",
       "      0.33112651109695435,\n",
       "      0.42531758546829224,\n",
       "      0.6050598621368408\n",
       "    ],\n",
       "    [\n",
       "      -0.8151780366897583,\n",
       "      -0.8605278730392456,\n",
       "      -0.9021719694137573\n",
       "    ],\n",
       "    [\n",
       "      0.5756203532218933,\n",
       "      -0.279142826795578,\n",
       "      -0.10808606445789337\n",
       "    ],\n",
       "    [\n",
       "      -0.35837414860725403,\n",
       "      -0.12738753855228424,\n",
       "      -0.4099779427051544\n",
       "    ],\n",
       "    [\n",
       "      0.20993082225322723,\n",
       "      -0.3724344074726105,\n",
       "      -0.46085381507873535\n",
       "    ],\n",
       "    [\n",
       "      0.5815994739532471,\n",
       "      0.6169049143791199,\n",
       "      0.6712881922721863\n",
       "    ],\n",
       "    [\n",
       "      -0.037142347544431686,\n",
       "      0.2509496212005615,\n",
       "      0.5934980511665344\n",
       "    ],\n",
       "    [\n",
       "      -0.5224048495292664,\n",
       "      0.15720880031585693,\n",
       "      -0.13347752392292023\n",
       "    ],\n",
       "    [\n",
       "      0.8516570329666138,\n",
       "      -0.28838855028152466,\n",
       "      -0.2811373770236969\n",
       "    ],\n",
       "    [\n",
       "      0.3303981423377991,\n",
       "      0.7609142661094666,\n",
       "      0.9427692294120789\n",
       "    ],\n",
       "    [\n",
       "      -0.526467502117157,\n",
       "      -0.19269751012325287,\n",
       "      -0.6645249724388123\n",
       "    ],\n",
       "    [\n",
       "      0.6592423915863037,\n",
       "      0.7726253271102905,\n",
       "      0.5242446660995483\n",
       "    ],\n",
       "    [\n",
       "      0.8826857805252075,\n",
       "      0.9557124376296997,\n",
       "      0.9768160581588745\n",
       "    ],\n",
       "    [\n",
       "      0.080816350877285,\n",
       "      0.05601746588945389,\n",
       "      0.0004282121662981808\n",
       "    ],\n",
       "    [\n",
       "      0.12476764619350433,\n",
       "      0.1192106381058693,\n",
       "      0.057267725467681885\n",
       "    ],\n",
       "    [\n",
       "      -0.3069612383842468,\n",
       "      -0.6384562849998474,\n",
       "      -0.646470844745636\n",
       "    ],\n",
       "    [\n",
       "      -0.8210628032684326,\n",
       "      0.1764705777168274,\n",
       "      0.88174968957901\n",
       "    ],\n",
       "    [\n",
       "      -0.029447853565216064,\n",
       "      -0.21727943420410156,\n",
       "      -0.4299236834049225\n",
       "    ],\n",
       "    [\n",
       "      -0.4459696114063263,\n",
       "      -0.30544278025627136,\n",
       "      -0.11068958044052124\n",
       "    ],\n",
       "    [\n",
       "      0.3415957987308502,\n",
       "      0.049382299184799194,\n",
       "      0.0515187606215477\n",
       "    ],\n",
       "    [\n",
       "      0.391489177942276,\n",
       "      0.526522696018219,\n",
       "      0.7385607957839966\n",
       "    ],\n",
       "    [\n",
       "      0.8275082111358643,\n",
       "      -0.7554028630256653,\n",
       "      -0.8247315287590027\n",
       "    ],\n",
       "    [\n",
       "      0.4855187237262726,\n",
       "      0.43647506833076477,\n",
       "      0.3769521713256836\n",
       "    ],\n",
       "    [\n",
       "      0.10949407517910004,\n",
       "      0.16393597424030304,\n",
       "      0.23906537890434265\n",
       "    ],\n",
       "    [\n",
       "      0.13651733100414276,\n",
       "      0.05571146309375763,\n",
       "      -0.7967619299888611\n",
       "    ],\n",
       "    [\n",
       "      -0.26615267992019653,\n",
       "      0.31644773483276367,\n",
       "      -0.1660955846309662\n",
       "    ],\n",
       "    [\n",
       "      -0.69644695520401,\n",
       "      -0.7076600790023804,\n",
       "      -0.7211990356445312\n",
       "    ],\n",
       "    [\n",
       "      0.13769978284835815,\n",
       "      0.18018724024295807,\n",
       "      -0.1801241785287857\n",
       "    ],\n",
       "    [\n",
       "      0.26187387108802795,\n",
       "      0.3039964437484741,\n",
       "      0.3465462625026703\n",
       "    ],\n",
       "    [\n",
       "      -0.8368983864784241,\n",
       "      0.18385538458824158,\n",
       "      0.5279856324195862\n",
       "    ],\n",
       "    [\n",
       "      -0.671728253364563,\n",
       "      -0.3264021873474121,\n",
       "      -0.53046053647995\n",
       "    ],\n",
       "    [\n",
       "      0.018821779638528824,\n",
       "      0.09530801326036453,\n",
       "      0.2028338462114334\n",
       "    ],\n",
       "    [\n",
       "      0.6071164608001709,\n",
       "      0.5858768820762634,\n",
       "      -0.1584533154964447\n",
       "    ],\n",
       "    [\n",
       "      0.5584195852279663,\n",
       "      0.03623456507921219,\n",
       "      -0.12032688409090042\n",
       "    ],\n",
       "    [\n",
       "      0.25074291229248047,\n",
       "      0.14844824373722076,\n",
       "      0.009652674198150635\n",
       "    ],\n",
       "    [\n",
       "      -0.8694665431976318,\n",
       "      -0.8634858727455139,\n",
       "      -0.839803159236908\n",
       "    ],\n",
       "    [\n",
       "      0.3917236626148224,\n",
       "      -0.047499798238277435,\n",
       "      -0.13375069200992584\n",
       "    ],\n",
       "    [\n",
       "      -0.7228668332099915,\n",
       "      -0.6486008763313293,\n",
       "      -0.46101221442222595\n",
       "    ],\n",
       "    [\n",
       "      0.18100985884666443,\n",
       "      0.38608551025390625,\n",
       "      0.628271758556366\n",
       "    ],\n",
       "    [\n",
       "      0.6831871867179871,\n",
       "      0.6719169616699219,\n",
       "      0.8747395873069763\n",
       "    ],\n",
       "    [\n",
       "      -0.7035294771194458,\n",
       "      0.9296077489852905,\n",
       "      -0.8525488972663879\n",
       "    ],\n",
       "    [\n",
       "      0.019319158047437668,\n",
       "      0.047794047743082047,\n",
       "      -0.09124830365180969\n",
       "    ],\n",
       "    [\n",
       "      0.21009762585163116,\n",
       "      0.21474285423755646,\n",
       "      0.2082643210887909\n",
       "    ],\n",
       "    [\n",
       "      0.20219512283802032,\n",
       "      0.11510763317346573,\n",
       "      0.08604258298873901\n",
       "    ],\n",
       "    [\n",
       "      -0.001771716051734984,\n",
       "      -0.1555626392364502,\n",
       "      -0.5461942553520203\n",
       "    ],\n",
       "    [\n",
       "      0.16574536263942719,\n",
       "      0.3201632499694824,\n",
       "      0.4023251235485077\n",
       "    ],\n",
       "    [\n",
       "      0.15098775923252106,\n",
       "      -0.019698454067111015,\n",
       "      -0.24036967754364014\n",
       "    ],\n",
       "    [\n",
       "      0.12424777448177338,\n",
       "      -0.3093182444572449,\n",
       "      -0.2761974036693573\n",
       "    ],\n",
       "    [\n",
       "      0.2556920349597931,\n",
       "      0.3668353259563446,\n",
       "      -0.15743565559387207\n",
       "    ],\n",
       "    [\n",
       "      -0.6734697222709656,\n",
       "      -0.6715599894523621,\n",
       "      -0.6557834148406982\n",
       "    ],\n",
       "    [\n",
       "      -0.9074482917785645,\n",
       "      -0.08410628139972687,\n",
       "      0.40000778436660767\n",
       "    ],\n",
       "    [\n",
       "      0.42929357290267944,\n",
       "      0.357200562953949,\n",
       "      0.2927454710006714\n",
       "    ],\n",
       "    [\n",
       "      -0.7549343705177307,\n",
       "      -0.7041350603103638,\n",
       "      -0.6599728465080261\n",
       "    ],\n",
       "    [\n",
       "      -0.31240081787109375,\n",
       "      -0.6062411665916443,\n",
       "      -0.7798004746437073\n",
       "    ],\n",
       "    [\n",
       "      -0.5995224714279175,\n",
       "      -0.5704464912414551,\n",
       "      -0.4742653965950012\n",
       "    ],\n",
       "    [\n",
       "      0.1815611869096756,\n",
       "      0.04752116650342941,\n",
       "      -0.005148107185959816\n",
       "    ],\n",
       "    [\n",
       "      0.2631436586380005,\n",
       "      0.48577186465263367,\n",
       "      0.6743332743644714\n",
       "    ],\n",
       "    [\n",
       "      0.449628621339798,\n",
       "      0.1334758698940277,\n",
       "      -0.22476890683174133\n",
       "    ],\n",
       "    [\n",
       "      -0.7262263298034668,\n",
       "      -0.8952340483665466,\n",
       "      -0.9364691972732544\n",
       "    ],\n",
       "    [\n",
       "      -0.8275038003921509,\n",
       "      -0.7674733400344849,\n",
       "      -0.6750351190567017\n",
       "    ],\n",
       "    [\n",
       "      -0.04179047793149948,\n",
       "      0.04246848449110985,\n",
       "      0.12192150205373764\n",
       "    ],\n",
       "    [\n",
       "      -0.6439576148986816,\n",
       "      -0.6421950459480286,\n",
       "      -0.5686083436012268\n",
       "    ],\n",
       "    [\n",
       "      0.365694522857666,\n",
       "      0.38971570134162903,\n",
       "      0.004083805251866579\n",
       "    ],\n",
       "    [\n",
       "      -0.11265650391578674,\n",
       "      -0.0031872179824858904,\n",
       "      0.041394345462322235\n",
       "    ],\n",
       "    [\n",
       "      -0.5328749418258667,\n",
       "      0.08787879347801208,\n",
       "      0.31642478704452515\n",
       "    ],\n",
       "    [\n",
       "      -0.512047529220581,\n",
       "      -0.08577389270067215,\n",
       "      0.09757570177316666\n",
       "    ],\n",
       "    [\n",
       "      -0.017580891028046608,\n",
       "      -0.20241889357566833,\n",
       "      -0.23954741656780243\n",
       "    ],\n",
       "    [\n",
       "      -0.22029943764209747,\n",
       "      0.2279697209596634,\n",
       "      0.6740188002586365\n",
       "    ],\n",
       "    [\n",
       "      0.5258346199989319,\n",
       "      0.3810059428215027,\n",
       "      0.27980920672416687\n",
       "    ],\n",
       "    [\n",
       "      0.26255419850349426,\n",
       "      0.17080917954444885,\n",
       "      -0.1069149374961853\n",
       "    ],\n",
       "    [\n",
       "      0.2701607644557953,\n",
       "      0.17936640977859497,\n",
       "      0.1253487765789032\n",
       "    ],\n",
       "    [\n",
       "      -0.1069243773818016,\n",
       "      0.14620104432106018,\n",
       "      0.21676896512508392\n",
       "    ],\n",
       "    [\n",
       "      0.40918177366256714,\n",
       "      0.4756438136100769,\n",
       "      -0.7484526038169861\n",
       "    ],\n",
       "    [\n",
       "      0.9131329655647278,\n",
       "      0.9117553234100342,\n",
       "      0.923079788684845\n",
       "    ],\n",
       "    [\n",
       "      0.2663901448249817,\n",
       "      -0.5995475053787231,\n",
       "      -0.3200604021549225\n",
       "    ],\n",
       "    [\n",
       "      -0.7784014940261841,\n",
       "      0.011764703318476677,\n",
       "      -0.508151650428772\n",
       "    ],\n",
       "    [\n",
       "      -0.8813919425010681,\n",
       "      -0.7712160348892212,\n",
       "      -0.07977991551160812\n",
       "    ],\n",
       "    [\n",
       "      0.12264435738325119,\n",
       "      -0.1356671154499054,\n",
       "      -0.21538789570331573\n",
       "    ],\n",
       "    [\n",
       "      -0.7406893968582153,\n",
       "      -0.82442706823349,\n",
       "      -0.8552130460739136\n",
       "    ],\n",
       "    [\n",
       "      0.4349195957183838,\n",
       "      0.38102126121520996,\n",
       "      -0.17550426721572876\n",
       "    ],\n",
       "    [\n",
       "      0.0998995453119278,\n",
       "      0.27294236421585083,\n",
       "      -0.07623804360628128\n",
       "    ],\n",
       "    [\n",
       "      -0.4556404650211334,\n",
       "      -0.6041439771652222,\n",
       "      -0.6755514740943909\n",
       "    ],\n",
       "    [\n",
       "      -0.8852124810218811,\n",
       "      -0.886846661567688,\n",
       "      -0.8974391222000122\n",
       "    ],\n",
       "    [\n",
       "      0.1085643544793129,\n",
       "      0.27788299322128296,\n",
       "      0.5110284090042114\n",
       "    ],\n",
       "    [\n",
       "      0.05066860839724541,\n",
       "      0.07404907047748566,\n",
       "      0.0850706696510315\n",
       "    ],\n",
       "    [\n",
       "      -0.0582071952521801,\n",
       "      -0.42424219846725464,\n",
       "      -0.6010069847106934\n",
       "    ],\n",
       "    [\n",
       "      0.7953606247901917,\n",
       "      0.5398732423782349,\n",
       "      0.2787138521671295\n",
       "    ],\n",
       "    [\n",
       "      0.07006106525659561,\n",
       "      -0.02700883150100708,\n",
       "      -0.046815067529678345\n",
       "    ],\n",
       "    [\n",
       "      -0.39510682225227356,\n",
       "      -0.41920360922813416,\n",
       "      -0.5861340761184692\n",
       "    ],\n",
       "    [\n",
       "      0.9032777547836304,\n",
       "      0.7805269360542297,\n",
       "      0.5566930174827576\n",
       "    ],\n",
       "    [\n",
       "      -0.2839460074901581,\n",
       "      -0.377784788608551,\n",
       "      -0.640014111995697\n",
       "    ],\n",
       "    [\n",
       "      0.6189615726470947,\n",
       "      0.7806115746498108,\n",
       "      0.9112246036529541\n",
       "    ],\n",
       "    [\n",
       "      -0.12736748158931732,\n",
       "      0.1799142211675644,\n",
       "      -0.3498200476169586\n",
       "    ],\n",
       "    [\n",
       "      -0.6782186031341553,\n",
       "      -0.4415965676307678,\n",
       "      -0.6616004705429077\n",
       "    ],\n",
       "    [\n",
       "      0.7154431939125061,\n",
       "      0.7134858965873718,\n",
       "      0.7111637592315674\n",
       "    ],\n",
       "    [\n",
       "      -0.40411651134490967,\n",
       "      -0.4058428704738617,\n",
       "      -0.7031396627426147\n",
       "    ],\n",
       "    [\n",
       "      0.6037266254425049,\n",
       "      0.2546057105064392,\n",
       "      -0.5865435600280762\n",
       "    ],\n",
       "    [\n",
       "      -0.030707459896802902,\n",
       "      -0.02565935254096985,\n",
       "      -0.029052134603261948\n",
       "    ],\n",
       "    [\n",
       "      0.5436667799949646,\n",
       "      0.4850476384162903,\n",
       "      0.29990139603614807\n",
       "    ],\n",
       "    [\n",
       "      -0.46743983030319214,\n",
       "      -0.47828489542007446,\n",
       "      -0.640227735042572\n",
       "    ],\n",
       "    [\n",
       "      -0.26479047536849976,\n",
       "      -0.2231954038143158,\n",
       "      -0.17111815512180328\n",
       "    ],\n",
       "    [\n",
       "      0.08362161368131638,\n",
       "      -0.34537962079048157,\n",
       "      -0.6366902589797974\n",
       "    ],\n",
       "    [\n",
       "      -0.6172446608543396,\n",
       "      -0.6172076463699341,\n",
       "      -0.6432552933692932\n",
       "    ],\n",
       "    [\n",
       "      -0.9108860492706299,\n",
       "      -0.29293063282966614,\n",
       "      0.2471790313720703\n",
       "    ],\n",
       "    [\n",
       "      0.8040373921394348,\n",
       "      0.9045073390007019,\n",
       "      0.9537075161933899\n",
       "    ],\n",
       "    [\n",
       "      -0.6513022184371948,\n",
       "      -0.24690783023834229,\n",
       "      0.09721279889345169\n",
       "    ],\n",
       "    [\n",
       "      -0.44596460461616516,\n",
       "      -0.6165546178817749,\n",
       "      -0.7865057587623596\n",
       "    ],\n",
       "    [\n",
       "      -0.11631107330322266,\n",
       "      -0.2858690619468689,\n",
       "      -0.14733800292015076\n",
       "    ],\n",
       "    [\n",
       "      0.8959205150604248,\n",
       "      0.776850163936615,\n",
       "      -0.4020872712135315\n",
       "    ],\n",
       "    [\n",
       "      0.3601052761077881,\n",
       "      0.3988553285598755,\n",
       "      0.49251291155815125\n",
       "    ],\n",
       "    [\n",
       "      -0.26731595396995544,\n",
       "      -0.26987895369529724,\n",
       "      -0.5473183393478394\n",
       "    ],\n",
       "    [\n",
       "      -0.47619685530662537,\n",
       "      -0.13198773562908173,\n",
       "      -0.1730058193206787\n",
       "    ],\n",
       "    [\n",
       "      -0.18654994666576385,\n",
       "      -0.08646202087402344,\n",
       "      -0.014718189835548401\n",
       "    ],\n",
       "    [\n",
       "      -0.4939781129360199,\n",
       "      -0.48898038268089294,\n",
       "      -0.7509691119194031\n",
       "    ],\n",
       "    [\n",
       "      0.4362282454967499,\n",
       "      0.2905654311180115,\n",
       "      0.08038295060396194\n",
       "    ],\n",
       "    [\n",
       "      0.5500906109809875,\n",
       "      0.6066980957984924,\n",
       "      0.18538472056388855\n",
       "    ],\n",
       "    [\n",
       "      -0.703561007976532,\n",
       "      -0.475872665643692,\n",
       "      -0.8547963500022888\n",
       "    ],\n",
       "    [\n",
       "      -0.32337120175361633,\n",
       "      -0.41644686460494995,\n",
       "      -0.44723910093307495\n",
       "    ],\n",
       "    [\n",
       "      -0.1228637844324112,\n",
       "      0.009513471275568008,\n",
       "      -0.3269110321998596\n",
       "    ],\n",
       "    [\n",
       "      -0.05532831326127052,\n",
       "      -0.2880820631980896,\n",
       "      -0.632136344909668\n",
       "    ],\n",
       "    [\n",
       "      0.01306537538766861,\n",
       "      0.1541440635919571,\n",
       "      0.31442636251449585\n",
       "    ],\n",
       "    [\n",
       "      -0.8547711372375488,\n",
       "      -0.7236877679824829,\n",
       "      -0.7856305837631226\n",
       "    ],\n",
       "    [\n",
       "      -0.33122292160987854,\n",
       "      -0.08996910601854324,\n",
       "      -0.013522649183869362\n",
       "    ],\n",
       "    [\n",
       "      0.19104398787021637,\n",
       "      0.49409112334251404,\n",
       "      0.29054054617881775\n",
       "    ],\n",
       "    [\n",
       "      -0.8246536254882812,\n",
       "      -0.8210623264312744,\n",
       "      -0.8379841446876526\n",
       "    ],\n",
       "    [\n",
       "      0.43059009313583374,\n",
       "      0.4535168409347534,\n",
       "      0.4542773365974426\n",
       "    ],\n",
       "    [\n",
       "      0.12398073077201843,\n",
       "      0.046275846660137177,\n",
       "      -0.0954698845744133\n",
       "    ],\n",
       "    [\n",
       "      0.33695098757743835,\n",
       "      0.545078456401825,\n",
       "      -0.09679412096738815\n",
       "    ],\n",
       "    [\n",
       "      0.2479376196861267,\n",
       "      -0.1584005504846573,\n",
       "      -0.45131367444992065\n",
       "    ],\n",
       "    [\n",
       "      0.5698011517524719,\n",
       "      -0.17435789108276367,\n",
       "      -0.3622756600379944\n",
       "    ],\n",
       "    [\n",
       "      -0.28326958417892456,\n",
       "      -0.552253007888794,\n",
       "      -0.511475145816803\n",
       "    ],\n",
       "    [\n",
       "      0.3965805172920227,\n",
       "      0.3034624457359314,\n",
       "      0.19590552151203156\n",
       "    ],\n",
       "    [\n",
       "      -0.12847775220870972,\n",
       "      -0.18598346412181854,\n",
       "      -0.2504082918167114\n",
       "    ],\n",
       "    [\n",
       "      0.7366775870323181,\n",
       "      0.7880193591117859,\n",
       "      0.8475437164306641\n",
       "    ],\n",
       "    [\n",
       "      -0.2861339747905731,\n",
       "      0.17025791108608246,\n",
       "      0.35271167755126953\n",
       "    ],\n",
       "    [\n",
       "      0.8075168132781982,\n",
       "      0.42018771171569824,\n",
       "      0.44603678584098816\n",
       "    ],\n",
       "    [\n",
       "      0.2363329976797104,\n",
       "      0.08777014166116714,\n",
       "      -0.28347423672676086\n",
       "    ],\n",
       "    [\n",
       "      -0.19575831294059753,\n",
       "      -0.3653962314128876,\n",
       "      -0.851190447807312\n",
       "    ],\n",
       "    [\n",
       "      -0.7790830135345459,\n",
       "      -0.7817010283470154,\n",
       "      -0.7980022430419922\n",
       "    ],\n",
       "    [\n",
       "      -0.4479544162750244,\n",
       "      -0.2243645042181015,\n",
       "      0.02055962011218071\n",
       "    ],\n",
       "    [\n",
       "      -0.4256124794483185,\n",
       "      0.07734345644712448,\n",
       "      0.8159140348434448\n",
       "    ],\n",
       "    [\n",
       "      -0.6993678212165833,\n",
       "      -0.4087635278701782,\n",
       "      -0.10601040720939636\n",
       "    ],\n",
       "    [\n",
       "      -0.1086140125989914,\n",
       "      0.5889700055122375,\n",
       "      0.9140363335609436\n",
       "    ],\n",
       "    [\n",
       "      -0.16228757798671722,\n",
       "      0.18070262670516968,\n",
       "      0.5169607996940613\n",
       "    ],\n",
       "    [\n",
       "      0.749700665473938,\n",
       "      0.6360651850700378,\n",
       "      0.4028933644294739\n",
       "    ],\n",
       "    [\n",
       "      -0.038698162883520126,\n",
       "      0.15878617763519287,\n",
       "      -0.019297881051898003\n",
       "    ],\n",
       "    [\n",
       "      0.26143696904182434,\n",
       "      0.5723651051521301,\n",
       "      0.7870779037475586\n",
       "    ],\n",
       "    [\n",
       "      0.6218112111091614,\n",
       "      0.505821704864502,\n",
       "      0.5277906656265259\n",
       "    ],\n",
       "    [\n",
       "      0.6569492220878601,\n",
       "      0.35561317205429077,\n",
       "      -0.1962130069732666\n",
       "    ],\n",
       "    [\n",
       "      0.8325598239898682,\n",
       "      0.10312407463788986,\n",
       "      -0.8632609844207764\n",
       "    ],\n",
       "    [\n",
       "      0.33171647787094116,\n",
       "      -0.21685273945331573,\n",
       "      -0.2871755063533783\n",
       "    ],\n",
       "    [\n",
       "      0.7343755960464478,\n",
       "      0.6765835881233215,\n",
       "      0.6062597036361694\n",
       "    ],\n",
       "    [\n",
       "      -0.38332948088645935,\n",
       "      -0.29144349694252014,\n",
       "      -0.6084114909172058\n",
       "    ],\n",
       "    [\n",
       "      0.8274057507514954,\n",
       "      0.8408244252204895,\n",
       "      0.8678486347198486\n",
       "    ],\n",
       "    [\n",
       "      0.24925090372562408,\n",
       "      0.07891064137220383,\n",
       "      -0.5096247792243958\n",
       "    ],\n",
       "    [\n",
       "      0.8545863032341003,\n",
       "      0.5584403872489929,\n",
       "      -0.13889265060424805\n",
       "    ],\n",
       "    [\n",
       "      -0.8311881422996521,\n",
       "      -0.7162603735923767,\n",
       "      -0.5443006157875061\n",
       "    ],\n",
       "    [\n",
       "      -0.08647031337022781,\n",
       "      -0.14451654255390167,\n",
       "      -0.18571554124355316\n",
       "    ],\n",
       "    [\n",
       "      0.6945480108261108,\n",
       "      0.38208863139152527,\n",
       "      0.11536668241024017\n",
       "    ],\n",
       "    [\n",
       "      -0.09427246451377869,\n",
       "      0.1726670265197754,\n",
       "      -0.5678977370262146\n",
       "    ],\n",
       "    [\n",
       "      0.0062930467538535595,\n",
       "      -0.08789492398500443,\n",
       "      -0.4074787497520447\n",
       "    ],\n",
       "    [\n",
       "      -0.549629271030426,\n",
       "      -0.5167146325111389,\n",
       "      -0.3297889828681946\n",
       "    ],\n",
       "    [\n",
       "      0.2673673629760742,\n",
       "      -0.19041119515895844,\n",
       "      -0.06358104199171066\n",
       "    ],\n",
       "    [\n",
       "      -0.7795626521110535,\n",
       "      -0.24047257006168365,\n",
       "      -0.7764204144477844\n",
       "    ],\n",
       "    [\n",
       "      0.3919673264026642,\n",
       "      0.19492611289024353,\n",
       "      0.13595764338970184\n",
       "    ],\n",
       "    [\n",
       "      -0.4196263551712036,\n",
       "      -0.8605479001998901,\n",
       "      -0.901617705821991\n",
       "    ],\n",
       "    [\n",
       "      -0.2352941483259201,\n",
       "      -0.8655933737754822,\n",
       "      -0.8846911191940308\n",
       "    ],\n",
       "    [\n",
       "      -0.4142451584339142,\n",
       "      -0.7462329268455505,\n",
       "      -0.7431978583335876\n",
       "    ],\n",
       "    [\n",
       "      0.23768068850040436,\n",
       "      0.4202742874622345,\n",
       "      0.796495795249939\n",
       "    ],\n",
       "    [\n",
       "      0.2594039738178253,\n",
       "      -0.039857808500528336,\n",
       "      -0.23500750958919525\n",
       "    ],\n",
       "    [\n",
       "      4.410944711707998e-06,\n",
       "      0.3582831621170044,\n",
       "      -0.42580118775367737\n",
       "    ],\n",
       "    [\n",
       "      -0.20306804776191711,\n",
       "      -0.4723082184791565,\n",
       "      -0.6012682318687439\n",
       "    ],\n",
       "    [\n",
       "      0.3004637062549591,\n",
       "      0.25049784779548645,\n",
       "      0.19274620711803436\n",
       "    ],\n",
       "    [\n",
       "      -0.23253419995307922,\n",
       "      -0.2656415104866028,\n",
       "      -0.3554094731807709\n",
       "    ],\n",
       "    [\n",
       "      0.18377049267292023,\n",
       "      0.22542840242385864,\n",
       "      0.3122413158416748\n",
       "    ],\n",
       "    [\n",
       "      0.8251723051071167,\n",
       "      0.3228589594364166,\n",
       "      -0.11610488593578339\n",
       "    ],\n",
       "    [\n",
       "      -0.12032292783260345,\n",
       "      -0.5216472744941711,\n",
       "      -0.7512328624725342\n",
       "    ],\n",
       "    [\n",
       "      0.3880918622016907,\n",
       "      0.13227270543575287,\n",
       "      -0.0909455418586731\n",
       "    ],\n",
       "    [\n",
       "      0.26736530661582947,\n",
       "      0.0636509582400322,\n",
       "      -0.07616468518972397\n",
       "    ],\n",
       "    [\n",
       "      -0.22423118352890015,\n",
       "      0.07944183051586151,\n",
       "      -0.45292964577674866\n",
       "    ],\n",
       "    [\n",
       "      -0.5266450047492981,\n",
       "      -0.30987870693206787,\n",
       "      -0.8572283387184143\n",
       "    ],\n",
       "    [\n",
       "      -0.1661577969789505,\n",
       "      0.06344860047101974,\n",
       "      -0.8015714883804321\n",
       "    ],\n",
       "    [\n",
       "      -0.18381772935390472,\n",
       "      -0.4403200149536133,\n",
       "      -0.46376028656959534\n",
       "    ],\n",
       "    [\n",
       "      0.6711505055427551,\n",
       "      0.7436378002166748,\n",
       "      0.7911440134048462\n",
       "    ],\n",
       "    [\n",
       "      0.03696923330426216,\n",
       "      0.1562359780073166,\n",
       "      -0.445999413728714\n",
       "    ],\n",
       "    [\n",
       "      0.5192427635192871,\n",
       "      0.6023258566856384,\n",
       "      0.7681598663330078\n",
       "    ],\n",
       "    [\n",
       "      0.4870956540107727,\n",
       "      0.19961246848106384,\n",
       "      0.011532967910170555\n",
       "    ],\n",
       "    [\n",
       "      0.5697644948959351,\n",
       "      -0.6889438033103943,\n",
       "      -0.6128193140029907\n",
       "    ],\n",
       "    [\n",
       "      -0.8247247934341431,\n",
       "      -0.6001573204994202,\n",
       "      -0.9008808732032776\n",
       "    ],\n",
       "    [\n",
       "      0.059214502573013306,\n",
       "      -0.7634263038635254,\n",
       "      -0.7111309170722961\n",
       "    ],\n",
       "    [\n",
       "      0.1423216313123703,\n",
       "      0.31812146306037903,\n",
       "      -0.26413336396217346\n",
       "    ],\n",
       "    [\n",
       "      -0.1638103425502777,\n",
       "      -0.07887032628059387,\n",
       "      -0.2320381999015808\n",
       "    ],\n",
       "    [\n",
       "      0.28969767689704895,\n",
       "      0.3724275231361389,\n",
       "      0.15289252996444702\n",
       "    ],\n",
       "    [\n",
       "      0.0041793230921030045,\n",
       "      -0.48368778824806213,\n",
       "      -0.4242289960384369\n",
       "    ],\n",
       "    [\n",
       "      0.5584286451339722,\n",
       "      0.5311771631240845,\n",
       "      0.41985878348350525\n",
       "    ],\n",
       "    [\n",
       "      -0.4475919008255005,\n",
       "      -0.20042356848716736,\n",
       "      0.5337203145027161\n",
       "    ],\n",
       "    [\n",
       "      0.5304732918739319,\n",
       "      0.3061673641204834,\n",
       "      0.43950074911117554\n",
       "    ],\n",
       "    [\n",
       "      0.6813693642616272,\n",
       "      0.7716038823127747,\n",
       "      0.2512742280960083\n",
       "    ],\n",
       "    [\n",
       "      -0.5219373106956482,\n",
       "      -0.4279458820819855,\n",
       "      -0.1875661313533783\n",
       "    ],\n",
       "    [\n",
       "      -0.41927874088287354,\n",
       "      -0.036482471972703934,\n",
       "      -0.5496504902839661\n",
       "    ],\n",
       "    [\n",
       "      0.5182844996452332,\n",
       "      -0.484189510345459,\n",
       "      -0.8203639388084412\n",
       "    ],\n",
       "    [\n",
       "      -0.508963406085968,\n",
       "      -0.5042758584022522,\n",
       "      -0.4699215888977051\n",
       "    ],\n",
       "    [\n",
       "      0.9184025526046753,\n",
       "      0.8484019041061401,\n",
       "      0.33833426237106323\n",
       "    ],\n",
       "    [\n",
       "      0.9437444806098938,\n",
       "      0.8895163536071777,\n",
       "      0.7109180092811584\n",
       "    ],\n",
       "    [\n",
       "      0.7086508870124817,\n",
       "      0.5615507364273071,\n",
       "      0.12094032764434814\n",
       "    ],\n",
       "    [\n",
       "      0.6170580983161926,\n",
       "      0.6768963932991028,\n",
       "      0.742318868637085\n",
       "    ],\n",
       "    [\n",
       "      -0.1199045330286026,\n",
       "      0.13261021673679352,\n",
       "      0.3690446615219116\n",
       "    ],\n",
       "    [\n",
       "      -0.17684566974639893,\n",
       "      -0.17434300482273102,\n",
       "      -0.1744946539402008\n",
       "    ],\n",
       "    [\n",
       "      -0.4894859194755554,\n",
       "      -0.35281237959861755,\n",
       "      -0.558021605014801\n",
       "    ],\n",
       "    [\n",
       "      0.10339447855949402,\n",
       "      -0.20046383142471313,\n",
       "      -0.7734662890434265\n",
       "    ],\n",
       "    [\n",
       "      0.8841200470924377,\n",
       "      0.8832088708877563,\n",
       "      0.8791232109069824\n",
       "    ],\n",
       "    [\n",
       "      0.1341663897037506,\n",
       "      0.5238715410232544,\n",
       "      -0.2299637794494629\n",
       "    ],\n",
       "    [\n",
       "      0.18523065745830536,\n",
       "      -0.04618392512202263,\n",
       "      -0.10783310979604721\n",
       "    ],\n",
       "    [\n",
       "      -0.47945553064346313,\n",
       "      -0.31917500495910645,\n",
       "      -0.23796002566814423\n",
       "    ],\n",
       "    [\n",
       "      -0.06797277182340622,\n",
       "      0.35138362646102905,\n",
       "      0.7296305298805237\n",
       "    ],\n",
       "    [\n",
       "      -0.06990627199411392,\n",
       "      -0.08354364335536957,\n",
       "      -0.13758940994739532\n",
       "    ],\n",
       "    [\n",
       "      -0.5663723945617676,\n",
       "      -0.44837072491645813,\n",
       "      -0.5650995969772339\n",
       "    ],\n",
       "    [\n",
       "      -0.20074671506881714,\n",
       "      -0.30797284841537476,\n",
       "      -0.451373815536499\n",
       "    ],\n",
       "    [\n",
       "      0.4359857141971588,\n",
       "      0.4774211645126343,\n",
       "      0.5535441040992737\n",
       "    ],\n",
       "    [\n",
       "      -0.5688455104827881,\n",
       "      -0.5204092264175415,\n",
       "      -0.6640347242355347\n",
       "    ],\n",
       "    [\n",
       "      0.2390112578868866,\n",
       "      -0.8670171499252319,\n",
       "      -0.8696901202201843\n",
       "    ],\n",
       "    [\n",
       "      -0.6326277256011963,\n",
       "      -0.7116203904151917,\n",
       "      -0.7878476977348328\n",
       "    ],\n",
       "    [\n",
       "      0.2262098342180252,\n",
       "      0.3781503140926361,\n",
       "      0.514397919178009\n",
       "    ],\n",
       "    [\n",
       "      0.7280123829841614,\n",
       "      0.8368320465087891,\n",
       "      0.9328539967536926\n",
       "    ],\n",
       "    [\n",
       "      -0.7567948698997498,\n",
       "      -0.7465658187866211,\n",
       "      -0.7405295968055725\n",
       "    ],\n",
       "    [\n",
       "      0.8170334100723267,\n",
       "      0.08413780480623245,\n",
       "      -0.48214709758758545\n",
       "    ],\n",
       "    [\n",
       "      0.862877368927002,\n",
       "      0.5352585315704346,\n",
       "      -0.7991546988487244\n",
       "    ],\n",
       "    [\n",
       "      -0.621928870677948,\n",
       "      -0.562061071395874,\n",
       "      -0.7660283446311951\n",
       "    ],\n",
       "    [\n",
       "      -0.6628718972206116,\n",
       "      -0.5574901103973389,\n",
       "      -0.046710383147001266\n",
       "    ],\n",
       "    [\n",
       "      0.4059176445007324,\n",
       "      -0.22110991179943085,\n",
       "      -0.5631029605865479\n",
       "    ],\n",
       "    [\n",
       "      -0.2655342221260071,\n",
       "      -0.10063542425632477,\n",
       "      -0.15453915297985077\n",
       "    ],\n",
       "    [\n",
       "      -0.6882926821708679,\n",
       "      -0.5326656699180603,\n",
       "      -0.3910406231880188\n",
       "    ],\n",
       "    [\n",
       "      -0.3209667205810547,\n",
       "      -0.26197776198387146,\n",
       "      -0.43213561177253723\n",
       "    ],\n",
       "    [\n",
       "      0.4604199230670929,\n",
       "      0.38297465443611145,\n",
       "      0.165588840842247\n",
       "    ],\n",
       "    [\n",
       "      -0.789753258228302,\n",
       "      -0.7486531138420105,\n",
       "      -0.8991166353225708\n",
       "    ],\n",
       "    [\n",
       "      -0.14251157641410828,\n",
       "      0.21082474291324615,\n",
       "      0.8753480315208435\n",
       "    ],\n",
       "    [\n",
       "      -0.17588567733764648,\n",
       "      -0.7455897331237793,\n",
       "      -0.7437286376953125\n",
       "    ],\n",
       "    [\n",
       "      0.15619508922100067,\n",
       "      -0.11082154512405396,\n",
       "      -0.33546683192253113\n",
       "    ],\n",
       "    [\n",
       "      0.0566534698009491,\n",
       "      -0.06090889498591423,\n",
       "      -0.2844159007072449\n",
       "    ],\n",
       "    [\n",
       "      -0.35421425104141235,\n",
       "      -0.1077323853969574,\n",
       "      0.1446550190448761\n",
       "    ],\n",
       "    [\n",
       "      -0.410139799118042,\n",
       "      -0.41112881898880005,\n",
       "      -0.4146181344985962\n",
       "    ],\n",
       "    [\n",
       "      0.45235970616340637,\n",
       "      -0.1573840230703354,\n",
       "      -0.834283709526062\n",
       "    ],\n",
       "    [\n",
       "      0.17508824169635773,\n",
       "      0.2110530585050583,\n",
       "      -0.3586375415325165\n",
       "    ],\n",
       "    [\n",
       "      -0.34698259830474854,\n",
       "      -0.21341188251972198,\n",
       "      -0.09654439240694046\n",
       "    ],\n",
       "    [\n",
       "      -0.5389053821563721,\n",
       "      -0.6091872453689575,\n",
       "      -0.7163283824920654\n",
       "    ],\n",
       "    [\n",
       "      0.07956509292125702,\n",
       "      -0.031822409480810165,\n",
       "      -0.15940652787685394\n",
       "    ],\n",
       "    [\n",
       "      -0.3426502048969269,\n",
       "      0.09453719109296799,\n",
       "      0.549526572227478\n",
       "    ],\n",
       "    [\n",
       "      0.3917296528816223,\n",
       "      0.008907724171876907,\n",
       "      -0.30424365401268005\n",
       "    ],\n",
       "    [\n",
       "      0.6411649584770203,\n",
       "      0.6219286322593689,\n",
       "      0.6035862565040588\n",
       "    ],\n",
       "    [\n",
       "      0.6803315877914429,\n",
       "      0.1710834503173828,\n",
       "      -0.2590431869029999\n",
       "    ],\n",
       "    [\n",
       "      0.3507155478000641,\n",
       "      0.18933378159999847,\n",
       "      0.010193992406129837\n",
       "    ],\n",
       "    [\n",
       "      0.36329159140586853,\n",
       "      0.16937659680843353,\n",
       "      -0.391919881105423\n",
       "    ],\n",
       "    [\n",
       "      -0.2764776051044464,\n",
       "      -0.347095787525177,\n",
       "      -0.40577244758605957\n",
       "    ],\n",
       "    [\n",
       "      0.0063696615397930145,\n",
       "      0.3818424940109253,\n",
       "      0.905337393283844\n",
       "    ],\n",
       "    [\n",
       "      0.15251892805099487,\n",
       "      0.4211327135562897,\n",
       "      0.022407291457057\n",
       "    ],\n",
       "    [\n",
       "      0.25898444652557373,\n",
       "      -0.08948460966348648,\n",
       "      -0.6144158244132996\n",
       "    ],\n",
       "    [\n",
       "      -0.0749160647392273,\n",
       "      -0.1953003704547882,\n",
       "      -0.33285605907440186\n",
       "    ],\n",
       "    [\n",
       "      -0.8303108811378479,\n",
       "      -0.3902731239795685,\n",
       "      -0.3073550760746002\n",
       "    ],\n",
       "    [\n",
       "      -0.08750402927398682,\n",
       "      -0.07856182754039764,\n",
       "      -0.05715752765536308\n",
       "    ],\n",
       "    [\n",
       "      -0.5891665816307068,\n",
       "      -0.2873663902282715,\n",
       "      -0.1156381368637085\n",
       "    ],\n",
       "    [\n",
       "      0.052701372653245926,\n",
       "      0.38162222504615784,\n",
       "      0.5678205490112305\n",
       "    ],\n",
       "    [\n",
       "      0.5728093385696411,\n",
       "      0.4326959252357483,\n",
       "      0.17818894982337952\n",
       "    ],\n",
       "    [\n",
       "      0.033164869993925095,\n",
       "      -0.16302891075611115,\n",
       "      -0.3282933235168457\n",
       "    ],\n",
       "    [\n",
       "      -0.26090341806411743,\n",
       "      -0.0005510819610208273,\n",
       "      -0.6438092589378357\n",
       "    ],\n",
       "    [\n",
       "      -0.14958596229553223,\n",
       "      -0.3920140266418457,\n",
       "      -0.6836188435554504\n",
       "    ],\n",
       "    [\n",
       "      -0.22344645857810974,\n",
       "      -0.1870509684085846,\n",
       "      -0.44519248604774475\n",
       "    ],\n",
       "    [\n",
       "      -0.7314428687095642,\n",
       "      -0.6415193676948547,\n",
       "      -0.30459868907928467\n",
       "    ],\n",
       "    [\n",
       "      0.6686636805534363,\n",
       "      0.4304346442222595,\n",
       "      -0.011660866439342499\n",
       "    ],\n",
       "    [\n",
       "      0.847905695438385,\n",
       "      0.6147153377532959,\n",
       "      0.655009925365448\n",
       "    ],\n",
       "    [\n",
       "      0.5191642642021179,\n",
       "      0.537932813167572,\n",
       "      0.5333459377288818\n",
       "    ],\n",
       "    [\n",
       "      -0.1298946589231491,\n",
       "      -0.25259557366371155,\n",
       "      -0.40116971731185913\n",
       "    ],\n",
       "    [\n",
       "      0.4965935945510864,\n",
       "      0.839232861995697,\n",
       "      0.9532268643379211\n",
       "    ],\n",
       "    [\n",
       "      0.25265082716941833,\n",
       "      0.26634716987609863,\n",
       "      0.26437729597091675\n",
       "    ],\n",
       "    [\n",
       "      0.5717523097991943,\n",
       "      0.5832701921463013,\n",
       "      0.5809952616691589\n",
       "    ],\n",
       "    [\n",
       "      -0.36550402641296387,\n",
       "      -0.3588101863861084,\n",
       "      -0.3724764585494995\n",
       "    ],\n",
       "    [\n",
       "      0.09444736689329147,\n",
       "      0.2112719565629959,\n",
       "      0.3880409896373749\n",
       "    ],\n",
       "    [\n",
       "      -0.6985506415367126,\n",
       "      -0.4201357960700989,\n",
       "      0.16628441214561462\n",
       "    ],\n",
       "    [\n",
       "      0.809805154800415,\n",
       "      0.6982276439666748,\n",
       "      0.5141733288764954\n",
       "    ],\n",
       "    [\n",
       "      -0.29925593733787537,\n",
       "      -0.17094896733760834,\n",
       "      -0.2989594340324402\n",
       "    ],\n",
       "    [\n",
       "      -0.033775947988033295,\n",
       "      -0.310159832239151,\n",
       "      -0.34480294585227966\n",
       "    ],\n",
       "    [\n",
       "      0.6758798956871033,\n",
       "      0.441753625869751,\n",
       "      0.24923746287822723\n",
       "    ],\n",
       "    [\n",
       "      -0.35855764150619507,\n",
       "      -0.2657393515110016,\n",
       "      -0.21003057062625885\n",
       "    ],\n",
       "    [\n",
       "      -0.4930666387081146,\n",
       "      -0.4461483359336853,\n",
       "      -0.3960783779621124\n",
       "    ],\n",
       "    [\n",
       "      -0.5915629267692566,\n",
       "      -0.14166218042373657,\n",
       "      0.2893812358379364\n",
       "    ],\n",
       "    [\n",
       "      0.5480598211288452,\n",
       "      -0.02691347897052765,\n",
       "      -0.5984203815460205\n",
       "    ],\n",
       "    [\n",
       "      0.8868749141693115,\n",
       "      0.8473175764083862,\n",
       "      0.8030847907066345\n",
       "    ],\n",
       "    [\n",
       "      0.08557762950658798,\n",
       "      0.4244401156902313,\n",
       "      0.7516128420829773\n",
       "    ],\n",
       "    [\n",
       "      0.10233782231807709,\n",
       "      0.274545818567276,\n",
       "      0.256741464138031\n",
       "    ],\n",
       "    [\n",
       "      -0.581525444984436,\n",
       "      -0.8821192979812622,\n",
       "      -0.9228616952896118\n",
       "    ],\n",
       "    [\n",
       "      0.029020698741078377,\n",
       "      0.21658559143543243,\n",
       "      0.12602707743644714\n",
       "    ],\n",
       "    [\n",
       "      -0.30120235681533813,\n",
       "      -0.1533181369304657,\n",
       "      -0.5427955985069275\n",
       "    ],\n",
       "    [\n",
       "      0.5009274482727051,\n",
       "      0.48171597719192505,\n",
       "      0.47619250416755676\n",
       "    ],\n",
       "    [\n",
       "      0.47403761744499207,\n",
       "      0.6451749205589294,\n",
       "      0.6711832284927368\n",
       "    ],\n",
       "    [\n",
       "      -0.3786222040653229,\n",
       "      -0.4587174355983734,\n",
       "      -0.5131866931915283\n",
       "    ],\n",
       "    [\n",
       "      -0.8491628766059875,\n",
       "      -0.9399452805519104,\n",
       "      -0.9578626751899719\n",
       "    ],\n",
       "    [\n",
       "      -0.5960239768028259,\n",
       "      -0.4241867959499359,\n",
       "      -0.7643530964851379\n",
       "    ],\n",
       "    [\n",
       "      -0.37123870849609375,\n",
       "      -0.3528949022293091,\n",
       "      -0.49114760756492615\n",
       "    ],\n",
       "    [\n",
       "      -0.5194482207298279,\n",
       "      -0.7201964855194092,\n",
       "      -0.8779999613761902\n",
       "    ],\n",
       "    [\n",
       "      -0.6902539730072021,\n",
       "      -0.7959086894989014,\n",
       "      -0.6496112942695618\n",
       "    ],\n",
       "    [\n",
       "      -0.07562510669231415,\n",
       "      0.024504806846380234,\n",
       "      -0.46301695704460144\n",
       "    ],\n",
       "    [\n",
       "      -0.877565324306488,\n",
       "      -0.430828332901001,\n",
       "      0.006420488469302654\n",
       "    ],\n",
       "    [\n",
       "      0.25796210765838623,\n",
       "      0.259445458650589,\n",
       "      0.47693929076194763\n",
       "    ],\n",
       "    [\n",
       "      -0.19203978776931763,\n",
       "      0.4261242747306824,\n",
       "      0.5270705223083496\n",
       "    ],\n",
       "    [\n",
       "      -0.3844175636768341,\n",
       "      -0.020532142370939255,\n",
       "      0.4136267602443695\n",
       "    ],\n",
       "    [\n",
       "      -0.6947348713874817,\n",
       "      -0.7752019762992859,\n",
       "      -0.7874815464019775\n",
       "    ],\n",
       "    [\n",
       "      0.8658393025398254,\n",
       "      0.7921077609062195,\n",
       "      0.7075756788253784\n",
       "    ],\n",
       "    [\n",
       "      -0.03359358012676239,\n",
       "      -0.0032838641200214624,\n",
       "      -0.21726182103157043\n",
       "    ],\n",
       "    [\n",
       "      0.13497254252433777,\n",
       "      -0.08375006914138794,\n",
       "      -0.45929840207099915\n",
       "    ],\n",
       "    [\n",
       "      -0.26721513271331787,\n",
       "      0.38139304518699646,\n",
       "      0.8757712841033936\n",
       "    ],\n",
       "    [\n",
       "      0.11953038722276688,\n",
       "      0.5874934792518616,\n",
       "      0.6712079048156738\n",
       "    ],\n",
       "    [\n",
       "      -0.7307054996490479,\n",
       "      -0.710096001625061,\n",
       "      -0.8239564299583435\n",
       "    ],\n",
       "    [\n",
       "      0.8117648363113403,\n",
       "      -0.05080314725637436,\n",
       "      -0.004829954821616411\n",
       "    ],\n",
       "    [\n",
       "      -0.589617133140564,\n",
       "      -0.5743433237075806,\n",
       "      -0.5764443874359131\n",
       "    ],\n",
       "    [\n",
       "      -0.5585607290267944,\n",
       "      -0.7995380163192749,\n",
       "      -0.7973589897155762\n",
       "    ],\n",
       "    [\n",
       "      -0.45065435767173767,\n",
       "      -0.4013029932975769,\n",
       "      -0.31873971223831177\n",
       "    ],\n",
       "    [\n",
       "      0.10054251551628113,\n",
       "      0.2636285722255707,\n",
       "      0.6881422996520996\n",
       "    ],\n",
       "    [\n",
       "      0.8072077631950378,\n",
       "      -0.5508065819740295,\n",
       "      -0.5025964379310608\n",
       "    ],\n",
       "    [\n",
       "      -0.35233694314956665,\n",
       "      -0.12778159976005554,\n",
       "      -0.8346442580223083\n",
       "    ],\n",
       "    [\n",
       "      -0.23788896203041077,\n",
       "      -0.2491414099931717,\n",
       "      -0.6852221488952637\n",
       "    ],\n",
       "    [\n",
       "      0.5601637363433838,\n",
       "      0.33189404010772705,\n",
       "      0.09012506157159805\n",
       "    ],\n",
       "    [\n",
       "      -0.5904211401939392,\n",
       "      -0.46565353870391846,\n",
       "      -0.43466705083847046\n",
       "    ],\n",
       "    [\n",
       "      -0.8602998852729797,\n",
       "      -0.5717481374740601,\n",
       "      -0.44156646728515625\n",
       "    ],\n",
       "    [\n",
       "      -0.30724796652793884,\n",
       "      -0.3099612295627594,\n",
       "      -0.32398149371147156\n",
       "    ],\n",
       "    [\n",
       "      0.3014434576034546,\n",
       "      0.5753018260002136,\n",
       "      0.9307978749275208\n",
       "    ],\n",
       "    [\n",
       "      -0.901679277420044,\n",
       "      -0.7673903703689575,\n",
       "      -0.38350850343704224\n",
       "    ],\n",
       "    [\n",
       "      -0.24826869368553162,\n",
       "      -0.49758121371269226,\n",
       "      -0.7222003936767578\n",
       "    ],\n",
       "    [\n",
       "      -0.02858002856373787,\n",
       "      0.26874157786369324,\n",
       "      0.4140278100967407\n",
       "    ],\n",
       "    [\n",
       "      -0.687764585018158,\n",
       "      -0.538379430770874,\n",
       "      -0.534829318523407\n",
       "    ],\n",
       "    [\n",
       "      0.5000979900360107,\n",
       "      0.49794813990592957,\n",
       "      0.03327731415629387\n",
       "    ],\n",
       "    [\n",
       "      0.14014378190040588,\n",
       "      0.512022852897644,\n",
       "      0.9017723202705383\n",
       "    ],\n",
       "    [\n",
       "      -0.17659704387187958,\n",
       "      -0.35234323143959045,\n",
       "      -0.3527551293373108\n",
       "    ],\n",
       "    [\n",
       "      0.5163330435752869,\n",
       "      -0.8715240955352783,\n",
       "      -0.8461443781852722\n",
       "    ],\n",
       "    [\n",
       "      0.9597784280776978,\n",
       "      0.9349061846733093,\n",
       "      0.8404857516288757\n",
       "    ],\n",
       "    [\n",
       "      0.40271055698394775,\n",
       "      0.5679798126220703,\n",
       "      -0.31350722908973694\n",
       "    ],\n",
       "    [\n",
       "      -0.6535230875015259,\n",
       "      -0.6329662203788757,\n",
       "      -0.8829557299613953\n",
       "    ],\n",
       "    [\n",
       "      -0.12881648540496826,\n",
       "      0.02529139816761017,\n",
       "      -0.10633862018585205\n",
       "    ],\n",
       "    [\n",
       "      -0.5647919178009033,\n",
       "      -0.7014521360397339,\n",
       "      -0.675957441329956\n",
       "    ],\n",
       "    [\n",
       "      0.15704724192619324,\n",
       "      0.6973430514335632,\n",
       "      0.9305424690246582\n",
       "    ],\n",
       "    [\n",
       "      0.3878478407859802,\n",
       "      0.5417211055755615,\n",
       "      0.6238986253738403\n",
       "    ],\n",
       "    [\n",
       "      0.31486886739730835,\n",
       "      0.23276633024215698,\n",
       "      -0.22907160222530365\n",
       "    ],\n",
       "    [\n",
       "      0.2343786507844925,\n",
       "      0.07411810755729675,\n",
       "      0.26075926423072815\n",
       "    ],\n",
       "    [\n",
       "      -0.8265745639801025,\n",
       "      0.1276153177022934,\n",
       "      0.059393130242824554\n",
       "    ],\n",
       "    [\n",
       "      0.43371477723121643,\n",
       "      0.14048443734645844,\n",
       "      -0.7748559713363647\n",
       "    ],\n",
       "    [\n",
       "      -0.2692457139492035,\n",
       "      0.013620681129395962,\n",
       "      0.25875985622406006\n",
       "    ],\n",
       "    [\n",
       "      -0.02501986362040043,\n",
       "      0.318763792514801,\n",
       "      -0.16803474724292755\n",
       "    ],\n",
       "    [\n",
       "      0.18032605946063995,\n",
       "      0.26465755701065063,\n",
       "      -0.5663284659385681\n",
       "    ],\n",
       "    [\n",
       "      0.5424352884292603,\n",
       "      0.7160876393318176,\n",
       "      0.8182350993156433\n",
       "    ],\n",
       "    [\n",
       "      -0.11874964833259583,\n",
       "      -0.11564076691865921,\n",
       "      -0.43408188223838806\n",
       "    ],\n",
       "    [\n",
       "      0.4049621820449829,\n",
       "      0.4641309678554535,\n",
       "      0.27158793807029724\n",
       "    ],\n",
       "    [\n",
       "      0.8023068904876709,\n",
       "      -0.3528258502483368,\n",
       "      0.4783160388469696\n",
       "    ],\n",
       "    [\n",
       "      -0.27317172288894653,\n",
       "      0.09456677734851837,\n",
       "      0.07421348243951797\n",
       "    ],\n",
       "    [\n",
       "      -0.1390434056520462,\n",
       "      0.004617467522621155,\n",
       "      0.17979542911052704\n",
       "    ],\n",
       "    [\n",
       "      0.39985713362693787,\n",
       "      0.6359505653381348,\n",
       "      0.812467098236084\n",
       "    ],\n",
       "    [\n",
       "      0.024408796802163124,\n",
       "      -0.5953179001808167,\n",
       "      -0.8567085266113281\n",
       "    ],\n",
       "    [\n",
       "      -0.5011078119277954,\n",
       "      -0.5504330396652222,\n",
       "      -0.6019151210784912\n",
       "    ],\n",
       "    [\n",
       "      0.2526206076145172,\n",
       "      -0.2878299653530121,\n",
       "      -0.6804863810539246\n",
       "    ],\n",
       "    [\n",
       "      -0.6287853717803955,\n",
       "      -0.6565880179405212,\n",
       "      -0.7183127403259277\n",
       "    ],\n",
       "    [\n",
       "      0.5216560363769531,\n",
       "      -0.42906203866004944,\n",
       "      -0.4035843014717102\n",
       "    ],\n",
       "    [\n",
       "      -0.17172972857952118,\n",
       "      -0.14767029881477356,\n",
       "      -0.5951231122016907\n",
       "    ],\n",
       "    [\n",
       "      -0.542864203453064,\n",
       "      -0.6117892861366272,\n",
       "      -0.6314513087272644\n",
       "    ],\n",
       "    [\n",
       "      -0.3827812969684601,\n",
       "      -0.29156258702278137,\n",
       "      -0.7587236762046814\n",
       "    ],\n",
       "    [\n",
       "      0.47409379482269287,\n",
       "      0.6714364290237427,\n",
       "      0.9321048855781555\n",
       "    ],\n",
       "    [\n",
       "      -0.5917093753814697,\n",
       "      -0.3855935335159302,\n",
       "      -0.3170311152935028\n",
       "    ],\n",
       "    [\n",
       "      0.30367207527160645,\n",
       "      -0.7085203528404236,\n",
       "      -0.6884373426437378\n",
       "    ],\n",
       "    [\n",
       "      0.607463002204895,\n",
       "      0.44477027654647827,\n",
       "      0.37944188714027405\n",
       "    ],\n",
       "    [\n",
       "      -0.5434997081756592,\n",
       "      -0.6743095517158508,\n",
       "      -0.779110312461853\n",
       "    ],\n",
       "    [\n",
       "      0.5268216133117676,\n",
       "      0.2740236222743988,\n",
       "      -0.3307431936264038\n",
       "    ],\n",
       "    [\n",
       "      0.2641393542289734,\n",
       "      -0.03398425504565239,\n",
       "      -0.37289056181907654\n",
       "    ],\n",
       "    [\n",
       "      -0.11731287091970444,\n",
       "      -0.24595773220062256,\n",
       "      -0.5200735926628113\n",
       "    ],\n",
       "    [\n",
       "      0.7649475932121277,\n",
       "      0.760320782661438,\n",
       "      0.7578855156898499\n",
       "    ],\n",
       "    [\n",
       "      -0.6559750437736511,\n",
       "      -0.7789433598518372,\n",
       "      -0.8842822909355164\n",
       "    ],\n",
       "    [\n",
       "      0.886038064956665,\n",
       "      0.6564491391181946,\n",
       "      0.1891615241765976\n",
       "    ],\n",
       "    [\n",
       "      -0.6598775386810303,\n",
       "      -0.302301824092865,\n",
       "      0.39183127880096436\n",
       "    ],\n",
       "    [\n",
       "      0.10474222898483276,\n",
       "      0.11633803695440292,\n",
       "      0.1397334337234497\n",
       "    ],\n",
       "    [\n",
       "      0.29360753297805786,\n",
       "      0.36898332834243774,\n",
       "      0.4225856363773346\n",
       "    ],\n",
       "    [\n",
       "      0.4249189496040344,\n",
       "      0.6796203255653381,\n",
       "      0.36297664046287537\n",
       "    ],\n",
       "    [\n",
       "      0.6751871109008789,\n",
       "      0.5418874025344849,\n",
       "      0.41607919335365295\n",
       "    ],\n",
       "    [\n",
       "      -0.37373998761177063,\n",
       "      -0.348294198513031,\n",
       "      -0.26444029808044434\n",
       "    ],\n",
       "    [\n",
       "      -0.20876768231391907,\n",
       "      -0.04786171764135361,\n",
       "      -0.5082888007164001\n",
       "    ],\n",
       "    [\n",
       "      0.07575385272502899,\n",
       "      0.07186414301395416,\n",
       "      -0.22131963074207306\n",
       "    ],\n",
       "    [\n",
       "      0.20403233170509338,\n",
       "      0.05782639980316162,\n",
       "      -0.1642524152994156\n",
       "    ],\n",
       "    [\n",
       "      0.8547954559326172,\n",
       "      0.36743852496147156,\n",
       "      -0.4031407833099365\n",
       "    ],\n",
       "    [\n",
       "      -0.140174999833107,\n",
       "      -0.12597081065177917,\n",
       "      -0.11109901964664459\n",
       "    ],\n",
       "    [\n",
       "      0.0036428782623261213,\n",
       "      0.01705816388130188,\n",
       "      0.029841415584087372\n",
       "    ],\n",
       "    [\n",
       "      0.5402044057846069,\n",
       "      0.07432081550359726,\n",
       "      -0.3797238767147064\n",
       "    ],\n",
       "    [\n",
       "      0.004338869825005531,\n",
       "      -0.1010223850607872,\n",
       "      -0.20412594079971313\n",
       "    ],\n",
       "    [\n",
       "      -0.29378676414489746,\n",
       "      -0.4300309419631958,\n",
       "      -0.5475382804870605\n",
       "    ],\n",
       "    [\n",
       "      0.6545546054840088,\n",
       "      0.5585492253303528,\n",
       "      0.30747750401496887\n",
       "    ],\n",
       "    [\n",
       "      0.6188711524009705,\n",
       "      0.6035498976707458,\n",
       "      0.48929306864738464\n",
       "    ],\n",
       "    [\n",
       "      0.29986414313316345,\n",
       "      0.4980831444263458,\n",
       "      0.5015079975128174\n",
       "    ],\n",
       "    [\n",
       "      -0.22464066743850708,\n",
       "      -0.17657655477523804,\n",
       "      -0.07300838083028793\n",
       "    ],\n",
       "    [\n",
       "      0.3251692056655884,\n",
       "      0.3183857202529907,\n",
       "      0.2722066044807434\n",
       "    ],\n",
       "    [\n",
       "      0.8105982542037964,\n",
       "      0.8071480989456177,\n",
       "      0.8013849258422852\n",
       "    ],\n",
       "    [\n",
       "      -0.8247439861297607,\n",
       "      -0.8224160671234131,\n",
       "      -0.7605470418930054\n",
       "    ],\n",
       "    [\n",
       "      0.4926680028438568,\n",
       "      0.7364184260368347,\n",
       "      -0.018585635349154472\n",
       "    ]\n",
       "  ],\n",
       "  \"do_color_quantize\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_processor_type\": \"ImageGPTImageProcessor\",\n",
       "  \"resample\": 2,\n",
       "  \"size\": {\n",
       "    \"height\": 32,\n",
       "    \"width\": 32\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor  = AutoImageProcessor.from_pretrained('openai/imagegpt-small')\n",
    "image_processor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "if \"height\" in image_processor.size:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = image_processor.size.get(\"longest_edge\")\n",
    "\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            # CenterCrop(crop_size),\n",
    "            ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    # x = torch.tensor(example_batch['pixel_values']).to(torch.int64)\n",
    "    # example_batch[\"pixel_values\"] = torch.tensor(example_batch['pixel_values']).to(device).long()\n",
    "    example_batch['pixel_values'] = [tensor.long() for tensor in example_batch['pixel_values']]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    # x = torch.tensor(example_batch['pixel_values']).to(torch.int64)\n",
    "    # example_batch[\"pixel_values\"] = torch.tensor(example_batch['pixel_values']).to(device).long()\n",
    "    example_batch['pixel_values'] = [tensor.long() for tensor in example_batch['pixel_values']]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up training into training + validation\n",
    "splits = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=64x64>,\n",
       " 'label': 1,\n",
       " 'pixel_values': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]])}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(train_ds[0]['label'])\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/imagegpt-small were not used when initializing ImageGPTForImageClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing ImageGPTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ImageGPTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ImageGPTForImageClassification were not initialized from the model checkpoint at openai/imagegpt-small and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    'openai/imagegpt-small', \n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # batch size for training and evaluation\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [03:17<?, ?it/s]\n",
      "c:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\models\\imagegpt\\modeling_imagegpt.py:1143: FutureWarning: The `pixel_values` argument is deprecated and will be removed in a future version, use `input_ids` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 201326592 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[188], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1644\u001b[0m )\n\u001b[1;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1650\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1935\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   1937\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1938\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1940\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1941\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1942\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1943\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1944\u001b[0m ):\n\u001b[0;32m   1945\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1946\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\trainer.py:2759\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2756\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2758\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2759\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2761\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2762\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\trainer.py:2784\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2782\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2783\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2784\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   2785\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2786\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2787\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\models\\imagegpt\\modeling_imagegpt.py:1158\u001b[0m, in \u001b[0;36mImageGPTForImageClassification.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     input_ids \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1156\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1158\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m   1159\u001b[0m     input_ids,\n\u001b[0;32m   1160\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1161\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1162\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1163\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1164\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1165\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1166\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1167\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1168\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1169\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1170\u001b[0m )\n\u001b[0;32m   1171\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1172\u001b[0m \u001b[39m# average-pool the hidden states along the sequence dimension\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\models\\imagegpt\\modeling_imagegpt.py:839\u001b[0m, in \u001b[0;36mImageGPTModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    829\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    830\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    831\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    836\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    837\u001b[0m     )\n\u001b[0;32m    838\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 839\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[0;32m    840\u001b[0m         hidden_states,\n\u001b[0;32m    841\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    842\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    843\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m    844\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    845\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    846\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    847\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    848\u001b[0m     )\n\u001b[0;32m    850\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    851\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\models\\imagegpt\\modeling_imagegpt.py:476\u001b[0m, in \u001b[0;36mImageGPTBlock.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    473\u001b[0m     outputs \u001b[39m=\u001b[39m outputs \u001b[39m+\u001b[39m cross_attn_outputs[\u001b[39m2\u001b[39m:]  \u001b[39m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[0;32m    475\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m--> 476\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_2(hidden_states)\n\u001b[0;32m    477\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(hidden_states)\n\u001b[0;32m    478\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\prime\\anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\models\\imagegpt\\modeling_imagegpt.py:171\u001b[0m, in \u001b[0;36mImageGPTLayerNorm.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m:\n\u001b[0;32m    168\u001b[0m     \u001b[39m# input is not mean centered\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    170\u001b[0m         tensor\n\u001b[1;32m--> 171\u001b[0m         \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39;49msquare(tensor), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps)\n\u001b[0;32m    172\u001b[0m         \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :]\n\u001b[0;32m    173\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 201326592 bytes."
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
